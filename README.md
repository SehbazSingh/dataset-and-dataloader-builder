# GPT Dataset V1 â€” PyTorch + tiktoken Tokenizer

This project provides a simple and efficient way to create training data for GPT-style language models using PyTorch and OpenAI's `tiktoken` tokenizer (GPT-2 encoding). It supports overlapping token sequences using a sliding window, suitable for next-token prediction training.

---

## ğŸ“¦ Features

- ğŸ” **Sliding Window Tokenization** with `max_length` and `stride`
- âš¡ **Efficient Batching** using PyTorch `DataLoader`
- ğŸ§  **Next-token Target Creation** (`input_ids`, `target_ids`)
- âœ‚ï¸ Customizable parameters: batch size, stride, sequence length

---

## ğŸ› ï¸ Installation

```bash
pip install torch tiktoken
